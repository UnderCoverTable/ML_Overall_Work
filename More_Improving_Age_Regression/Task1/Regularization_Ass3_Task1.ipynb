{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LassoCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the raw images\n",
    "path_raw = r\"C:\\Users\\Sameed\\Desktop\\ML_Overall_Work\\More_Improving_Age_Regression\\Dataset\\UTKFace (Reduced and sameed + arham)\"\n",
    "\n",
    "#Get the names of all the images\n",
    "images = os.listdir(path_raw)\n",
    "\n",
    "#Store the paths of all the cropped images we will save later\n",
    "paths_cropped = []\n",
    "\n",
    "img_dim = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_ages = []\n",
    "features = []\n",
    "\n",
    "#Store all the paths for the unknown images\n",
    "for unknowns in enumerate(images):\n",
    "    \n",
    "    age = \"\"\n",
    "    for i in range(0,6):\n",
    "        if(unknowns[1][i] == \"_\"):\n",
    "            break\n",
    "        else:\n",
    "            age += unknowns[1][i]\n",
    "\n",
    "    unknown_ages.append(int(age))\n",
    "\n",
    "    path = path_raw + \"\\\\\" + unknowns[1]\n",
    "    image = cv2.imread(path)\n",
    "\n",
    "    img_resize = cv2.resize(image, (img_dim,img_dim))\n",
    "    features.append(img_resize.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(features)\n",
    "y = np.array(unknown_ages)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l1; total time=   8.6s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l1; total time=   8.2s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l1; total time=   7.2s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l1; total time=   5.0s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l1; total time=   4.3s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l2; total time=   1.5s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l2; total time=   1.5s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l2; total time=   1.6s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l2; total time=   2.0s\n",
      "[CV] END .........eta0=1e-05, loss=squared_error, penalty=l2; total time=   1.2s\n",
      "[CV] END .eta0=1e-05, loss=squared_error, penalty=elasticnet; total time=   5.2s\n",
      "[CV] END .eta0=1e-05, loss=squared_error, penalty=elasticnet; total time=   4.3s\n",
      "[CV] END .eta0=1e-05, loss=squared_error, penalty=elasticnet; total time=   6.2s\n",
      "[CV] END .eta0=1e-05, loss=squared_error, penalty=elasticnet; total time=   7.2s\n",
      "[CV] END .eta0=1e-05, loss=squared_error, penalty=elasticnet; total time=   4.6s\n",
      "[CV] END .......eta0=1e-05, loss=squared_error, penalty=None; total time=   1.5s\n",
      "[CV] END .......eta0=1e-05, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END .......eta0=1e-05, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END .......eta0=1e-05, loss=squared_error, penalty=None; total time=   1.8s\n",
      "[CV] END .......eta0=1e-05, loss=squared_error, penalty=None; total time=   2.0s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l1; total time=   4.1s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l1; total time=   3.6s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l1; total time=   3.7s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l1; total time=   7.1s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l1; total time=   4.0s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l2; total time=   1.2s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l2; total time=   1.6s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l2; total time=   1.4s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END ...eta0=1e-05, loss=epsilon_insensitive, penalty=l2; total time=   2.1s\n",
      "[CV] END eta0=1e-05, loss=epsilon_insensitive, penalty=elasticnet; total time=   5.6s\n",
      "[CV] END eta0=1e-05, loss=epsilon_insensitive, penalty=elasticnet; total time=   4.1s\n",
      "[CV] END eta0=1e-05, loss=epsilon_insensitive, penalty=elasticnet; total time=   5.2s\n",
      "[CV] END eta0=1e-05, loss=epsilon_insensitive, penalty=elasticnet; total time=   2.0s\n",
      "[CV] END eta0=1e-05, loss=epsilon_insensitive, penalty=elasticnet; total time=   6.8s\n",
      "[CV] END .eta0=1e-05, loss=epsilon_insensitive, penalty=None; total time=   1.8s\n",
      "[CV] END .eta0=1e-05, loss=epsilon_insensitive, penalty=None; total time=   1.0s\n",
      "[CV] END .eta0=1e-05, loss=epsilon_insensitive, penalty=None; total time=   1.6s\n",
      "[CV] END .eta0=1e-05, loss=epsilon_insensitive, penalty=None; total time=   1.6s\n",
      "[CV] END .eta0=1e-05, loss=epsilon_insensitive, penalty=None; total time=   2.4s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l1; total time=   3.2s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l1; total time=   4.4s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l1; total time=   2.4s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l1; total time=   3.0s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l1; total time=   2.3s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l2; total time=   1.7s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l2; total time=   0.9s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l2; total time=   1.2s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l2; total time=   0.6s\n",
      "[CV] END .................eta0=1e-05, loss=huber, penalty=l2; total time=   0.7s\n",
      "[CV] END .........eta0=1e-05, loss=huber, penalty=elasticnet; total time=   1.9s\n",
      "[CV] END .........eta0=1e-05, loss=huber, penalty=elasticnet; total time=   4.4s\n",
      "[CV] END .........eta0=1e-05, loss=huber, penalty=elasticnet; total time=   4.0s\n",
      "[CV] END .........eta0=1e-05, loss=huber, penalty=elasticnet; total time=   5.2s\n",
      "[CV] END .........eta0=1e-05, loss=huber, penalty=elasticnet; total time=   1.8s\n",
      "[CV] END ...............eta0=1e-05, loss=huber, penalty=None; total time=   0.6s\n",
      "[CV] END ...............eta0=1e-05, loss=huber, penalty=None; total time=   1.4s\n",
      "[CV] END ...............eta0=1e-05, loss=huber, penalty=None; total time=   1.0s\n",
      "[CV] END ...............eta0=1e-05, loss=huber, penalty=None; total time=   1.6s\n",
      "[CV] END ...............eta0=1e-05, loss=huber, penalty=None; total time=   0.8s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l1; total time=   4.9s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.3s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l1; total time=   7.5s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l1; total time=   3.9s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l1; total time=   4.5s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.4s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.5s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.0s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.1s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.3s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.7s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   8.1s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.2s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   9.8s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=None; total time=   2.6s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=None; total time=   0.8s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=None; total time=   1.6s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=None; total time=   1.4s\n",
      "[CV] END eta0=1e-05, loss=squared_epsilon_insensitive, penalty=None; total time=   2.4s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l1; total time=   7.5s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l1; total time=   6.7s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l1; total time=   3.1s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l1; total time=   3.9s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l1; total time=   7.4s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l2; total time=   2.2s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l2; total time=   2.4s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l2; total time=   2.9s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l2; total time=   1.9s\n",
      "[CV] END ........eta0=0.0001, loss=squared_error, penalty=l2; total time=   1.5s\n",
      "[CV] END eta0=0.0001, loss=squared_error, penalty=elasticnet; total time=   9.7s\n",
      "[CV] END eta0=0.0001, loss=squared_error, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END eta0=0.0001, loss=squared_error, penalty=elasticnet; total time=   3.7s\n",
      "[CV] END eta0=0.0001, loss=squared_error, penalty=elasticnet; total time=   8.3s\n",
      "[CV] END eta0=0.0001, loss=squared_error, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END ......eta0=0.0001, loss=squared_error, penalty=None; total time=   2.0s\n",
      "[CV] END ......eta0=0.0001, loss=squared_error, penalty=None; total time=   2.8s\n",
      "[CV] END ......eta0=0.0001, loss=squared_error, penalty=None; total time=   2.1s\n",
      "[CV] END ......eta0=0.0001, loss=squared_error, penalty=None; total time=   2.7s\n",
      "[CV] END ......eta0=0.0001, loss=squared_error, penalty=None; total time=   2.4s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l1; total time=   8.2s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l1; total time=  10.4s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l1; total time=   3.9s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l1; total time=   8.3s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l1; total time=   5.4s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l2; total time=   2.5s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l2; total time=   2.0s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l2; total time=   3.5s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l2; total time=   2.9s\n",
      "[CV] END ..eta0=0.0001, loss=epsilon_insensitive, penalty=l2; total time=   3.2s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=elasticnet; total time=  10.4s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=elasticnet; total time=   6.1s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=elasticnet; total time=   8.1s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=elasticnet; total time=   8.4s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=elasticnet; total time=   9.9s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=None; total time=   2.5s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=None; total time=   2.9s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=None; total time=   1.5s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=None; total time=   1.7s\n",
      "[CV] END eta0=0.0001, loss=epsilon_insensitive, penalty=None; total time=   2.6s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l1; total time=   4.0s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l1; total time=   3.0s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l1; total time=   4.2s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l1; total time=   4.3s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l1; total time=   3.1s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l2; total time=   1.0s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l2; total time=   1.4s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l2; total time=   0.6s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l2; total time=   1.7s\n",
      "[CV] END ................eta0=0.0001, loss=huber, penalty=l2; total time=   1.0s\n",
      "[CV] END ........eta0=0.0001, loss=huber, penalty=elasticnet; total time=   4.8s\n",
      "[CV] END ........eta0=0.0001, loss=huber, penalty=elasticnet; total time=   4.4s\n",
      "[CV] END ........eta0=0.0001, loss=huber, penalty=elasticnet; total time=   4.7s\n",
      "[CV] END ........eta0=0.0001, loss=huber, penalty=elasticnet; total time=   3.8s\n",
      "[CV] END ........eta0=0.0001, loss=huber, penalty=elasticnet; total time=   4.1s\n",
      "[CV] END ..............eta0=0.0001, loss=huber, penalty=None; total time=   1.6s\n",
      "[CV] END ..............eta0=0.0001, loss=huber, penalty=None; total time=   1.1s\n",
      "[CV] END ..............eta0=0.0001, loss=huber, penalty=None; total time=   0.8s\n",
      "[CV] END ..............eta0=0.0001, loss=huber, penalty=None; total time=   1.9s\n",
      "[CV] END ..............eta0=0.0001, loss=huber, penalty=None; total time=   1.3s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.0s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.6s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.1s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l1; total time=   3.7s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.7s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.9s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.8s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.8s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.6s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.3s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   4.7s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   5.2s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.1s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=None; total time=   2.8s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=None; total time=   2.0s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=None; total time=   1.9s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=None; total time=   1.9s\n",
      "[CV] END eta0=0.0001, loss=squared_epsilon_insensitive, penalty=None; total time=   2.5s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l1; total time=   6.8s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l1; total time=   5.4s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l1; total time=   9.9s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l1; total time=   3.2s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l1; total time=   5.3s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l2; total time=   1.5s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l2; total time=   2.0s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l2; total time=   1.1s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l2; total time=   1.4s\n",
      "[CV] END .........eta0=0.001, loss=squared_error, penalty=l2; total time=   2.5s\n",
      "[CV] END .eta0=0.001, loss=squared_error, penalty=elasticnet; total time=   6.1s\n",
      "[CV] END .eta0=0.001, loss=squared_error, penalty=elasticnet; total time=   3.6s\n",
      "[CV] END .eta0=0.001, loss=squared_error, penalty=elasticnet; total time=   5.0s\n",
      "[CV] END .eta0=0.001, loss=squared_error, penalty=elasticnet; total time=   6.6s\n",
      "[CV] END .eta0=0.001, loss=squared_error, penalty=elasticnet; total time=   7.0s\n",
      "[CV] END .......eta0=0.001, loss=squared_error, penalty=None; total time=   1.2s\n",
      "[CV] END .......eta0=0.001, loss=squared_error, penalty=None; total time=   2.6s\n",
      "[CV] END .......eta0=0.001, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END .......eta0=0.001, loss=squared_error, penalty=None; total time=   2.6s\n",
      "[CV] END .......eta0=0.001, loss=squared_error, penalty=None; total time=   2.2s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l1; total time=  10.9s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l1; total time=  14.6s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l1; total time=   7.8s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l1; total time=  10.2s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l1; total time=   7.6s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l2; total time=   2.8s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l2; total time=   3.5s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l2; total time=   2.2s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l2; total time=   3.9s\n",
      "[CV] END ...eta0=0.001, loss=epsilon_insensitive, penalty=l2; total time=   3.7s\n",
      "[CV] END eta0=0.001, loss=epsilon_insensitive, penalty=elasticnet; total time=  11.2s\n",
      "[CV] END eta0=0.001, loss=epsilon_insensitive, penalty=elasticnet; total time=  10.9s\n",
      "[CV] END eta0=0.001, loss=epsilon_insensitive, penalty=elasticnet; total time=   9.9s\n",
      "[CV] END eta0=0.001, loss=epsilon_insensitive, penalty=elasticnet; total time=  10.4s\n",
      "[CV] END eta0=0.001, loss=epsilon_insensitive, penalty=elasticnet; total time=   9.7s\n",
      "[CV] END .eta0=0.001, loss=epsilon_insensitive, penalty=None; total time=   3.3s\n",
      "[CV] END .eta0=0.001, loss=epsilon_insensitive, penalty=None; total time=   3.1s\n",
      "[CV] END .eta0=0.001, loss=epsilon_insensitive, penalty=None; total time=   2.0s\n",
      "[CV] END .eta0=0.001, loss=epsilon_insensitive, penalty=None; total time=   3.4s\n",
      "[CV] END .eta0=0.001, loss=epsilon_insensitive, penalty=None; total time=   3.0s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l1; total time=   9.9s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l1; total time=   8.2s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l1; total time=  11.2s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l1; total time=  10.1s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l1; total time=  11.1s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l2; total time=   2.9s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l2; total time=   2.9s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l2; total time=   3.2s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l2; total time=   3.0s\n",
      "[CV] END .................eta0=0.001, loss=huber, penalty=l2; total time=   1.4s\n",
      "[CV] END .........eta0=0.001, loss=huber, penalty=elasticnet; total time=  10.4s\n",
      "[CV] END .........eta0=0.001, loss=huber, penalty=elasticnet; total time=   7.0s\n",
      "[CV] END .........eta0=0.001, loss=huber, penalty=elasticnet; total time=   9.1s\n",
      "[CV] END .........eta0=0.001, loss=huber, penalty=elasticnet; total time=  11.4s\n",
      "[CV] END .........eta0=0.001, loss=huber, penalty=elasticnet; total time=   9.8s\n",
      "[CV] END ...............eta0=0.001, loss=huber, penalty=None; total time=   3.9s\n",
      "[CV] END ...............eta0=0.001, loss=huber, penalty=None; total time=   3.8s\n",
      "[CV] END ...............eta0=0.001, loss=huber, penalty=None; total time=   2.5s\n",
      "[CV] END ...............eta0=0.001, loss=huber, penalty=None; total time=   2.7s\n",
      "[CV] END ...............eta0=0.001, loss=huber, penalty=None; total time=   2.9s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.2s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.6s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.1s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l1; total time=   8.2s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l1; total time=   4.7s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.3s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.5s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.7s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.8s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.9s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   3.9s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   7.1s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   7.7s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   5.9s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   5.4s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=None; total time=   1.3s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=None; total time=   2.4s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=None; total time=   1.7s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=None; total time=   2.1s\n",
      "[CV] END eta0=0.001, loss=squared_epsilon_insensitive, penalty=None; total time=   1.4s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l1; total time=   5.2s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l1; total time=   5.5s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l1; total time=   6.8s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l1; total time=   7.5s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l1; total time=   7.3s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l2; total time=   2.1s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l2; total time=   1.9s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l2; total time=   1.7s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l2; total time=   2.1s\n",
      "[CV] END ..........eta0=0.01, loss=squared_error, penalty=l2; total time=   1.1s\n",
      "[CV] END ..eta0=0.01, loss=squared_error, penalty=elasticnet; total time=   5.8s\n",
      "[CV] END ..eta0=0.01, loss=squared_error, penalty=elasticnet; total time=   4.2s\n",
      "[CV] END ..eta0=0.01, loss=squared_error, penalty=elasticnet; total time=   6.1s\n",
      "[CV] END ..eta0=0.01, loss=squared_error, penalty=elasticnet; total time=   7.8s\n",
      "[CV] END ..eta0=0.01, loss=squared_error, penalty=elasticnet; total time=   5.7s\n",
      "[CV] END ........eta0=0.01, loss=squared_error, penalty=None; total time=   2.2s\n",
      "[CV] END ........eta0=0.01, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END ........eta0=0.01, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END ........eta0=0.01, loss=squared_error, penalty=None; total time=   2.3s\n",
      "[CV] END ........eta0=0.01, loss=squared_error, penalty=None; total time=   1.6s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l1; total time=  10.2s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l1; total time=   8.0s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l1; total time=  12.9s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l1; total time=   6.1s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l1; total time=   6.7s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l2; total time=   3.9s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l2; total time=   2.3s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l2; total time=   2.5s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l2; total time=   4.1s\n",
      "[CV] END ....eta0=0.01, loss=epsilon_insensitive, penalty=l2; total time=   2.8s\n",
      "[CV] END eta0=0.01, loss=epsilon_insensitive, penalty=elasticnet; total time=   7.6s\n",
      "[CV] END eta0=0.01, loss=epsilon_insensitive, penalty=elasticnet; total time=   5.0s\n",
      "[CV] END eta0=0.01, loss=epsilon_insensitive, penalty=elasticnet; total time=  10.8s\n",
      "[CV] END eta0=0.01, loss=epsilon_insensitive, penalty=elasticnet; total time=   8.6s\n",
      "[CV] END eta0=0.01, loss=epsilon_insensitive, penalty=elasticnet; total time=  10.2s\n",
      "[CV] END ..eta0=0.01, loss=epsilon_insensitive, penalty=None; total time=   2.3s\n",
      "[CV] END ..eta0=0.01, loss=epsilon_insensitive, penalty=None; total time=   2.9s\n",
      "[CV] END ..eta0=0.01, loss=epsilon_insensitive, penalty=None; total time=   2.7s\n",
      "[CV] END ..eta0=0.01, loss=epsilon_insensitive, penalty=None; total time=   3.0s\n",
      "[CV] END ..eta0=0.01, loss=epsilon_insensitive, penalty=None; total time=   3.4s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l1; total time=   8.1s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l1; total time=   9.9s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l1; total time=   9.9s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l1; total time=   6.1s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l1; total time=   9.3s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l2; total time=   2.3s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l2; total time=   1.4s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l2; total time=   2.8s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l2; total time=   3.7s\n",
      "[CV] END ..................eta0=0.01, loss=huber, penalty=l2; total time=   3.2s\n",
      "[CV] END ..........eta0=0.01, loss=huber, penalty=elasticnet; total time=   6.6s\n",
      "[CV] END ..........eta0=0.01, loss=huber, penalty=elasticnet; total time=  11.1s\n",
      "[CV] END ..........eta0=0.01, loss=huber, penalty=elasticnet; total time=  10.1s\n",
      "[CV] END ..........eta0=0.01, loss=huber, penalty=elasticnet; total time=   9.8s\n",
      "[CV] END ..........eta0=0.01, loss=huber, penalty=elasticnet; total time=   6.8s\n",
      "[CV] END ................eta0=0.01, loss=huber, penalty=None; total time=   2.1s\n",
      "[CV] END ................eta0=0.01, loss=huber, penalty=None; total time=   2.9s\n",
      "[CV] END ................eta0=0.01, loss=huber, penalty=None; total time=   2.1s\n",
      "[CV] END ................eta0=0.01, loss=huber, penalty=None; total time=   2.4s\n",
      "[CV] END ................eta0=0.01, loss=huber, penalty=None; total time=   2.7s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.0s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l1; total time=   4.9s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l1; total time=   7.1s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.2s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.0s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l2; total time=   3.1s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.3s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.3s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   7.7s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.2s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.1s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   8.8s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   4.8s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=None; total time=   2.1s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=None; total time=   1.6s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=None; total time=   2.2s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=None; total time=   2.4s\n",
      "[CV] END eta0=0.01, loss=squared_epsilon_insensitive, penalty=None; total time=   2.3s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l1; total time=   7.9s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l1; total time=   4.0s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l1; total time=   9.1s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l1; total time=   5.2s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l1; total time=   5.6s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l2; total time=   2.6s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l2; total time=   2.3s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l2; total time=   1.9s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l2; total time=   2.2s\n",
      "[CV] END ...........eta0=0.1, loss=squared_error, penalty=l2; total time=   2.4s\n",
      "[CV] END ...eta0=0.1, loss=squared_error, penalty=elasticnet; total time=   5.4s\n",
      "[CV] END ...eta0=0.1, loss=squared_error, penalty=elasticnet; total time=   5.8s\n",
      "[CV] END ...eta0=0.1, loss=squared_error, penalty=elasticnet; total time=   4.3s\n",
      "[CV] END ...eta0=0.1, loss=squared_error, penalty=elasticnet; total time=   7.0s\n",
      "[CV] END ...eta0=0.1, loss=squared_error, penalty=elasticnet; total time=   5.5s\n",
      "[CV] END .........eta0=0.1, loss=squared_error, penalty=None; total time=   3.3s\n",
      "[CV] END .........eta0=0.1, loss=squared_error, penalty=None; total time=   1.9s\n",
      "[CV] END .........eta0=0.1, loss=squared_error, penalty=None; total time=   1.3s\n",
      "[CV] END .........eta0=0.1, loss=squared_error, penalty=None; total time=   1.6s\n",
      "[CV] END .........eta0=0.1, loss=squared_error, penalty=None; total time=   2.0s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l1; total time=   5.0s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l1; total time=   9.4s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l1; total time=   6.2s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l1; total time=   7.5s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l1; total time=   6.9s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l2; total time=   3.1s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l2; total time=   3.5s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l2; total time=   4.1s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l2; total time=   3.0s\n",
      "[CV] END .....eta0=0.1, loss=epsilon_insensitive, penalty=l2; total time=   1.7s\n",
      "[CV] END eta0=0.1, loss=epsilon_insensitive, penalty=elasticnet; total time=   8.4s\n",
      "[CV] END eta0=0.1, loss=epsilon_insensitive, penalty=elasticnet; total time=  11.1s\n",
      "[CV] END eta0=0.1, loss=epsilon_insensitive, penalty=elasticnet; total time=   9.5s\n",
      "[CV] END eta0=0.1, loss=epsilon_insensitive, penalty=elasticnet; total time=   7.5s\n",
      "[CV] END eta0=0.1, loss=epsilon_insensitive, penalty=elasticnet; total time=   9.5s\n",
      "[CV] END ...eta0=0.1, loss=epsilon_insensitive, penalty=None; total time=   3.0s\n",
      "[CV] END ...eta0=0.1, loss=epsilon_insensitive, penalty=None; total time=   2.3s\n",
      "[CV] END ...eta0=0.1, loss=epsilon_insensitive, penalty=None; total time=   5.0s\n",
      "[CV] END ...eta0=0.1, loss=epsilon_insensitive, penalty=None; total time=   2.6s\n",
      "[CV] END ...eta0=0.1, loss=epsilon_insensitive, penalty=None; total time=   3.1s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l1; total time=  11.2s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l1; total time=  11.4s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l1; total time=   8.9s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l1; total time=  11.3s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l1; total time=  14.0s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l2; total time=   3.2s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l2; total time=   2.5s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l2; total time=   3.7s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l2; total time=   2.7s\n",
      "[CV] END ...................eta0=0.1, loss=huber, penalty=l2; total time=   2.6s\n",
      "[CV] END ...........eta0=0.1, loss=huber, penalty=elasticnet; total time=   7.8s\n",
      "[CV] END ...........eta0=0.1, loss=huber, penalty=elasticnet; total time=   9.2s\n",
      "[CV] END ...........eta0=0.1, loss=huber, penalty=elasticnet; total time=   6.2s\n",
      "[CV] END ...........eta0=0.1, loss=huber, penalty=elasticnet; total time=   7.8s\n",
      "[CV] END ...........eta0=0.1, loss=huber, penalty=elasticnet; total time=  10.7s\n",
      "[CV] END .................eta0=0.1, loss=huber, penalty=None; total time=   2.5s\n",
      "[CV] END .................eta0=0.1, loss=huber, penalty=None; total time=   1.8s\n",
      "[CV] END .................eta0=0.1, loss=huber, penalty=None; total time=   2.0s\n",
      "[CV] END .................eta0=0.1, loss=huber, penalty=None; total time=   3.1s\n",
      "[CV] END .................eta0=0.1, loss=huber, penalty=None; total time=   2.0s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l1; total time=   5.0s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.2s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.8s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l1; total time=   7.7s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l1; total time=   6.6s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.3s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.1s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.0s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l2; total time=   1.8s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=l2; total time=   2.1s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   4.1s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   4.5s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   9.1s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=elasticnet; total time=   6.5s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=None; total time=   2.5s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=None; total time=   1.8s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=None; total time=   2.9s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=None; total time=   1.7s\n",
      "[CV] END eta0=0.1, loss=squared_epsilon_insensitive, penalty=None; total time=   1.4s\n",
      "Best parameters: {'eta0': 1e-05, 'loss': 'huber', 'penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"penalty\" : ['l1', 'l2', 'elasticnet', None],\n",
    "    \"eta0\" : [0.00001,0.0001,0.001,0.01,0.1],\n",
    "    \"loss\" : [\"squared_error\",\"epsilon_insensitive\",\"huber\",\"squared_epsilon_insensitive\"]\n",
    "}\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "grid_search = GridSearchCV(sgd_reg, param_grid, cv=None, verbose=2)\n",
    "grid_search.fit(x,y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'eta0': 1e-05, 'loss': 'huber', 'penalty': 'elasticnet'}\n",
      "intercept= [0.00017668]\n",
      "coef= [ 1.86925527e-03  3.49269860e-03  2.52216876e-03 ... -1.56876892e-03\n",
      " -2.05656354e-05 -2.78842787e-03]\n",
      "Testing: \n",
      " R2 = 0.29457232759622687 \n",
      " MSE = 289.1994432175238 \n",
      "\n",
      " Validation: \n",
      " R2 = 0.5292935074592306 \n",
      " Mse = 176.66010459543662\n"
     ]
    }
   ],
   "source": [
    "#Best parameters: {'eta0': 1e-05, 'loss': 'huber', 'penalty': 'elasticnet'}\n",
    "print(\"Best parameters: {'eta0': 1e-05, 'loss': 'huber', 'penalty': 'elasticnet'}\")\n",
    "\n",
    "\n",
    "sgd_regressor_best = SGDRegressor(loss=\"huber\",penalty=\"elasticnet\", eta0=0.00001)\n",
    "sgd_regressor_best.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_sgd_test = sgd_regressor_best.predict(x_test)\n",
    "y_pred_sgd_val = sgd_regressor_best.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_sgd_test)\n",
    "r2_test = r2_score(y_test, y_pred_sgd_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_sgd_val)\n",
    "r2_val = r2_score(y_train, y_pred_sgd_val)\n",
    "\n",
    "print(\"intercept=\", sgd_regressor_best.intercept_)\n",
    "print(\"coef=\", sgd_regressor_best.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept= [2.02057265e+10]\n",
      "coef= [-1.14815640e+12 -1.15861570e+12 -3.04693126e+12 ...  2.28172565e+12\n",
      "  1.19453832e+12  3.58577793e+12]\n",
      "Testing: \n",
      " R2 = -1.035651136196906e+29 \n",
      " MSE = 4.24578937958515e+31 \n",
      "\n",
      " Validation: \n",
      " R2 = -1.1636907481571606e+29 \n",
      " Mse = 4.367429227001371e+31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd_regressor = SGDRegressor()\n",
    "sgd_regressor.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_sgd_test = sgd_regressor.predict(x_test)\n",
    "y_pred_sgd_val = sgd_regressor.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_sgd_test)\n",
    "r2_test = r2_score(y_test, y_pred_sgd_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_sgd_val)\n",
    "r2_val = r2_score(y_train, y_pred_sgd_val)\n",
    "\n",
    "print(\"intercept=\", sgd_regressor.intercept_)\n",
    "print(\"coef=\", sgd_regressor.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "intercept= 36.26796266134528\n",
      "coef= [ 0.16289873 -0.1838536   0.0060522  ... -0.07434907  0.15201617\n",
      " -0.02620253]\n",
      "Testing: \n",
      " R2 = 0.9999999995863119 \n",
      " MSE = 1.6959690617641762e-07 \n",
      "\n",
      " Validation: \n",
      " R2 = 0.9997056573759304 \n",
      " Mse = 0.1104692617991223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ridge = RidgeCV(alphas=[0.00001,0.0001,0.001,0.01,0.1])\n",
    "\n",
    "ridge.fit(x, y)\n",
    "\n",
    "\n",
    "y_pred_ridge_test = ridge.predict(x_test)\n",
    "y_pred_ridge_val = ridge.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_ridge_test)\n",
    "r2_test = r2_score(y_test, y_pred_ridge_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_ridge_val)\n",
    "r2_val = r2_score(y_train, y_pred_ridge_val)\n",
    "\n",
    "print(\"Best alpha:\", ridge.alpha_)\n",
    "print(\"intercept=\", ridge.intercept_)\n",
    "print(\"coef=\", ridge.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+04, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.306e+03, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.263e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.506e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.381e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.691e+04, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+03, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.397e+02, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e+02, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.695e+04, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.318e+03, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.940e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.324e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+04, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+03, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.525e+02, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.219e+02, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.165e+01, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.751e+03, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.313e+03, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.196e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "intercept= 36.90205533030036\n",
      "coef= [ 0.04658213 -0.07383511  0.03491625 ... -0.03334419  0.08342905\n",
      " -0.02869899]\n",
      "Testing: \n",
      " R2 = 0.9595304655752072 \n",
      " MSE = 16.591023120827465 \n",
      "\n",
      " Validation: \n",
      " R2 = 0.9558670546849198 \n",
      " Mse = 16.5634654694963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+04, tolerance: 8.125e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lasso = LassoCV(alphas=[0.00001,0.0001,0.001,0.01,0.1])\n",
    "\n",
    "lasso.fit(x, y)\n",
    "\n",
    "y_pred_lasso_test = lasso.predict(x_test)\n",
    "y_pred_lasso_val = lasso.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_lasso_test)\n",
    "r2_test = r2_score(y_test, y_pred_lasso_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_lasso_val)\n",
    "r2_val = r2_score(y_train, y_pred_lasso_val)\n",
    "\n",
    "print(\"Best alpha:\", lasso.alpha_)\n",
    "print(\"intercept=\", lasso.intercept_)\n",
    "print(\"coef=\", lasso.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+04, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+03, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.410e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.276e+02, tolerance: 6.052e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.234e+04, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.204e+03, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+02, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.257e+02, tolerance: 7.844e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.242e+04, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+03, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.128e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.192e+02, tolerance: 7.631e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.238e+04, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e+03, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.578e+02, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 7.180e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.915e+03, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.571e+03, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.553e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.038e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+02, tolerance: 2.801e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "intercept= 37.283125135252135\n",
      "coef= [ 0.04234396 -0.07627742  0.04282558 ... -0.03048236  0.08887105\n",
      " -0.03369789]\n",
      "Testing: \n",
      " R2 = 0.9675068316575418 \n",
      " MSE = 13.32100590977883 \n",
      "\n",
      " Validation: \n",
      " R2 = 0.9632593880518443 \n",
      " Mse = 13.789060598307701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.330e+04, tolerance: 8.125e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elastic = ElasticNetCV(alphas=[0.00001,0.0001,0.001,0.01,0.1])\n",
    "\n",
    "elastic.fit(x, y)\n",
    "\n",
    "y_pred_elastic_test = elastic.predict(x_test)\n",
    "y_pred_elastic_val = elastic.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_elastic_test)\n",
    "r2_test = r2_score(y_test, y_pred_elastic_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_elastic_val)\n",
    "r2_val = r2_score(y_train, y_pred_elastic_val)\n",
    "\n",
    "print(\"Best alpha:\", elastic.alpha_)\n",
    "print(\"intercept=\", elastic.intercept_)\n",
    "print(\"coef=\", elastic.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept = -1491147386.0275977\n",
      "Coef = [ -5577736.17283156 -25315870.41436087  51127113.75116897 ...\n",
      "  -9119928.6615228   15413030.38322135  17892538.55384046]\n",
      "Testing: \n",
      " R2 = -5.184394522872705e+17 \n",
      " MSE = 2.1254113895557284e+20 \n",
      "\n",
      " Validation: \n",
      " R2 = 0.9997056575655837 \n",
      " Mse = 0.11046919062063132\n"
     ]
    }
   ],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_ols_test = ols.predict(x_test)\n",
    "y_pred_ols_val = ols.predict(x_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_ols_test)\n",
    "r2_test = r2_score(y_test, y_pred_ols_test)\n",
    "\n",
    "mse_val = mean_squared_error(y_train, y_pred_ols_val)\n",
    "r2_val = r2_score(y_train, y_pred_ols_val)\n",
    "\n",
    "print(\"Intercept =\", ols.intercept_)\n",
    "print(\"Coef =\", ols.coef_)\n",
    "\n",
    "print(f\"Testing: \\n R2 = {r2_test} \\n MSE = {mse_test} \\n\\n Validation: \\n R2 = {r2_val} \\n Mse = {mse_val}\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL SCORES \n",
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD REGRESSOR:\n",
    "\n",
    "Vanilla: \n",
    "\n",
    "intercept= [2.02057265e+10]\n",
    "\n",
    "coef= [-1.14815640e+12 -1.15861570e+12 -3.04693126e+12 ...  2.28172565e+12\n",
    "  1.19453832e+12  3.58577793e+12]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = -1.035651136196906e+29 \n",
    "\n",
    " MSE = 4.24578937958515e+31 \n",
    "\n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = -1.1636907481571606e+29 \n",
    "\n",
    " Mse = 4.367429227001371e+31\n",
    "\n",
    " -----------------------------------\n",
    " \n",
    "\n",
    "Best parameters: {'eta0': 1e-05, 'loss': 'huber', 'penalty': 'elasticnet'}\n",
    "\n",
    "intercept= [0.00017668]\n",
    "\n",
    "coef= [ 1.86925527e-03  3.49269860e-03  2.52216876e-03 ... -1.56876892e-03\n",
    " -2.05656354e-05 -2.78842787e-03]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = 0.29457232759622687 \n",
    "\n",
    " MSE = 289.1994432175238 \n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = 0.5292935074592306 \n",
    "\n",
    " Mse = 176.66010459543662\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "RidgeCV\n",
    "\n",
    "Best alpha: 0.1\n",
    "\n",
    "intercept= 36.26796266134528\n",
    "\n",
    "coef= [ 0.16289873 -0.1838536   0.0060522  ... -0.07434907  0.15201617\n",
    " -0.02620253]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = 0.9999999995863119 \n",
    "\n",
    " MSE = 1.6959690617641762e-07 \n",
    "\n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = 0.9997056573759304 \n",
    "\n",
    " Mse = 0.1104692617991223"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "LassoCV\n",
    "\n",
    "Best alpha: 0.1\n",
    "\n",
    "intercept= 36.90205533030036\n",
    "\n",
    "coef= [ 0.04658213 -0.07383511  0.03491625 ... -0.03334419  0.08342905\n",
    " -0.02869899]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = 0.9595304655752072 \n",
    "\n",
    " MSE = 16.591023120827465 \n",
    "\n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = 0.9558670546849198 \n",
    "\n",
    " Mse = 16.5634654694963"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "ElasticNetCV\n",
    "\n",
    "Best alpha: 0.1\n",
    "\n",
    "intercept= 37.283125135252135\n",
    "\n",
    "coef= [ 0.04234396 -0.07627742  0.04282558 ... -0.03048236  0.08887105\n",
    " -0.03369789]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = 0.9675068316575418 \n",
    "\n",
    " MSE = 13.32100590977883 \n",
    "\n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = 0.9632593880518443 \n",
    "\n",
    " Mse = 13.789060598307701"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "OLS\n",
    "\n",
    "Intercept = -1491147386.0275977\n",
    "\n",
    "Coef = [ -5577736.17283156 -25315870.41436087  51127113.75116897 ...\n",
    "  -9119928.6615228   15413030.38322135  17892538.55384046]\n",
    "\n",
    "Testing: \n",
    "\n",
    " R2 = -5.184394522872705e+17 \n",
    "\n",
    " MSE = 2.1254113895557284e+20 \n",
    "\n",
    " Validation: \n",
    "\n",
    " R2 = 0.9997056575655837 \n",
    "\n",
    " Mse = 0.11046919062063132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the best model\n",
    "\n",
    "#RidgeCV\n",
    "pickle.dump(ridge, open('ridge.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
